Deep Dive into Core Functions
_chunk_by_page() - Intelligent Document Segmentation
This function implements a sophisticated page-based chunking strategy specifically designed for legal documents. It takes lists of paragraph and table dictionaries as input, where each dictionary contains text content and metadata including page numbers.
The function begins by initializing empty dictionaries for page grouping and a set to track signature pages. It performs a first pass through all paragraphs searching for signature indicators using a predefined list of keywords including "IN WITNESS WHEREOF", "AGREED TO AND ACCEPTED", "/s/", and role identifiers like "Name:" and "Title:". Any page containing these keywords gets flagged as a signature page. This two-pass approach allows the function to identify all signature pages before constructing chunks, enabling intelligent recombination.
In the second pass, the function groups paragraphs by page number, concatenating all text from paragraphs and tables on the same page. The critical logic comes when handling signature pages: if signatures are found and include_signatures is enabled in configuration, the function combines the first page (containing party definitions and agreement context) with all signature pages into a single chunk. This design choice reflects the understanding that party identification often requires both the definition section and signature blocks to fully extract signing parties with their roles and representations.
The function outputs a list of text strings, where each string represents either a single page or the special combined first-page-plus-signatures chunk. This chunking strategy ensures that entity extraction models receive complete semantic units with necessary context, rather than fragmented information split across chunks.
_chunk_by_chars() - Sliding Window Text Segmentation
This alternative chunking method processes documents as continuous text streams when page boundaries aren't meaningful or available. It accepts the same paragraph list input but ignores page metadata.
The function concatenates all paragraph text into a single string, then implements a sliding window approach with configurable overlap. Starting at position 0, it extracts chunks of max_chars length (default 1000 characters). The overlap mechanism is crucial: instead of moving the window by the full chunk size, it moves by (chunk_size - overlap), ensuring that text near chunk boundaries appears in multiple chunks. This redundancy prevents the loss of entities that might span chunk boundaries.
The function continues sliding until it reaches the end of the text, with the final chunk potentially being shorter than max_chars. Empty or whitespace-only chunks are filtered out. The output is a list of overlapping text segments that cover the entire document. This approach trades increased processing (due to overlap) for improved recall, as entities won't be split mid-extraction.
_get_tokens() - Cached Text Tokenization
This function implements efficient tokenization with caching to avoid redundant processing of repeated text segments. It takes a text string as input and returns a list of normalized tokens.
The function first checks the TokenCache for previously tokenized versions of the input text. If found, it returns the cached result immediately, avoiding recomputation. For new text, it performs simple whitespace-based tokenization (splitting on spaces, tabs, newlines), then normalizes each token using the _normalize_token function. The normalized tokens are stored in the cache before returning.
The caching strategy is critical for performance because documents often contain repetitive sections (boilerplate language, standard clauses) that would otherwise be tokenized multiple times. The cache uses an LRU eviction policy to bound memory usage while keeping frequently accessed tokens available.
_normalize_token() - Token Standardization
This function, decorated with @lru_cache(maxsize=10000), normalizes individual tokens for consistent matching. It takes a single token string and returns its normalized form.
The normalization process involves converting to lowercase and stripping whitespace. Additionally, it implements light stemming by removing trailing 's' from words longer than 3 characters (unless they end in 'ss', avoiding false stemming of words like "class" or "across"). This stemming helps match singular/plural variations without the complexity of full morphological analysis.
The LRU cache decoration means the function maintains a dictionary of the 10,000 most recently processed tokens and their normalized forms. Given that documents typically have limited vocabulary, this cache size ensures most tokens are processed only once, providing O(1) lookup for repeated tokens.
TokenCache Class - LRU Token Cache Management
This class implements a least-recently-used cache specifically for tokenized text segments. It maintains both a dictionary for storage and a deque for tracking access order.
The get() method checks if requested text exists in the cache. If found, it removes the text from its current position in the access order deque and appends it to the end (marking it as most recently used), then returns the cached tokens. This O(n) deque operation is acceptable because the cache size is limited (default 100 entries).
The put() method adds new tokenized text to the cache. If the cache is at capacity, it removes the least recently used entry (leftmost in the deque) before adding the new entry. This ensures the cache maintains a bounded size while keeping the most useful entries.
_progressive_match() - Multi-Stage Extraction Validation
This is the core validation function that determines whether an LLM's extraction actually appears in the source text. It takes an extraction dictionary (containing the extracted text and metadata) and the source text chunk, returning either the validated extraction with match metadata or None if no match is found.
The function first handles special cases where extractions contain an "extractions" array (used for multi-value extractions like multiple signing parties). If this array exists and contains data, it's immediately accepted as valid without text matching, as these represent structured extractions that don't require validation against source text.
For text-based extractions, the function implements a three-stage matching pipeline:
Stage 1 performs exact substring matching using Python's in operator. This is the fastest check and catches cases where the LLM returned verbatim text. If successful, the extraction is tagged with match_type: "exact" and returned immediately.
Stage 2 checks if the configuration specifies exact_only: true. If so, the function returns None, enforcing strict extraction requirements. This is useful for high-precision scenarios where paraphrasing is unacceptable.
Stage 3 implements token-based fuzzy matching with multiple passes. The function tokenizes both the extraction and source text (using the cached tokenization functions), then attempts matching with progressively relaxed thresholds. Each pass uses a different threshold from configuration (e.g., fuzzy_threshold_1: 0.90, fuzzy_threshold_2: 0.70), allowing the system to prefer high-confidence matches while still catching lower-confidence valid extractions.
_token_based_match() - Optimized Sequence Matching
This function implements the core fuzzy matching algorithm, finding the best match between extracted tokens and source tokens. It takes two token lists and a threshold, returning a similarity ratio between 0 and 1.
The function begins with optimization checks: if either list is empty, it returns 0. If the source is shorter than the extraction multiplied by the threshold, it cannot possibly contain a good match and returns 0 immediately. These early exits prevent unnecessary computation.
The sliding window approach is key to efficiency. Instead of comparing the extraction against every possible subsequence of the source (O(n²)), it uses windows of varying sizes. For each window size (starting from the extraction length up to twice that length or the source length), it slides through the source text.
The critical optimization uses Counter objects for rapid intersection checking. Before computing expensive SequenceMatcher ratios, the function computes the token overlap between the extraction and current window using counter intersection (extract_counts & window_counts). Only if this overlap exceeds the minimum required tokens does it proceed to full sequence matching.
The window sliding is optimized using a deque and incremental counter updates. As the window slides, it removes the leftmost token from the counter and adds the new rightmost token, maintaining O(1) updates rather than reconstructing the counter each time.
The function includes early termination: if it finds a match with ratio ≥ 0.95, it returns immediately rather than searching for marginally better matches. This reflects the understanding that very high confidence matches don't benefit from further optimization.
_process_row() - Row-Level Extraction Pipeline
This function processes a single document (represented as a row) for a specific entity type. It takes a row dictionary containing paragraphs and tables, plus an entity configuration object.
The function first handles potential JSON string inputs, parsing them into Python objects if necessary. It then chunks the document using the configured chunker, which may use page-based or character-based strategies. If no chunks are produced (empty document), it returns an empty list.
The function implements intelligent batching logic: if the number of chunks exceeds a configured threshold (default 10) and batching is enabled, it formats all prompts at once and sends them to the model's batch inference endpoint. Otherwise, it processes chunks individually. This adaptive approach balances latency and throughput based on document size.
For each chunk, the function formats a prompt using the entity's template (which may include specific instructions for extracting that entity type), sends it to the model, parses the response, and validates it using progressive matching. Only validated extractions are included in the results list, providing quality control at the extraction level.
Design Interconnections
The tokenization, chunking, and fuzzy matching systems work together to create a robust extraction pipeline. Chunking ensures the LLM sees semantically complete units (like full pages or page combinations for contracts), maximizing extraction accuracy. The progressive matching system then validates these extractions, with tokenization and caching making fuzzy matching computationally feasible even for large documents with many chunks.
The multi-pass fuzzy matching with decreasing thresholds reflects a key insight: extraction quality varies, and a system that only accepts perfect matches (high threshold) will miss valid but reformatted extractions, while one that accepts everything (low threshold) will include false positives. The progressive approach finds a middle ground, preferring high-quality matches but accepting lower-quality ones when necessary.
The caching strategies (both token-level and text-level) recognize that document processing involves significant repetition - the same boilerplate appears across documents, similar chunks are processed for multiple entities, and common tokens appear thousands of times. By caching aggressively with bounded memory usage, the system achieves near-linear scaling with document size rather than quadratic scaling that naive implementations would exhibit.


Deep Dive: The Progressive Matching Pipeline
The progressive matching system represents the critical validation layer between raw LLM outputs and trusted extractions. This subsystem addresses a fundamental challenge in LLM-based extraction: models often generate text that captures the semantic meaning correctly but may introduce variations in phrasing, formatting, or minor details that don't exist in the source document.
The Extraction-to-Validation Flow
When an LLM processes a chunk and returns an extraction, the response typically contains JSON with a "text" field representing what was extracted. This extraction must be validated against the original chunk text to ensure the model didn't hallucinate or significantly alter the content. The validation occurs immediately after parsing the model response in _process_row, where each extraction is passed through _progressive_match along with its source chunk.
The chunk text at this point has already been processed by the Chunker class, meaning it's a continuous string of text that may span multiple pages (in page-based chunking) or a specific character window (in character-based chunking). This chunked text serves as the ground truth against which all extractions are validated.
Stage 1: Exact Match Detection
The _progressive_match method begins by attempting exact substring matching:
pythonextract_text = extraction.get("text", "")
if extract_text and extract_text in text:
    extraction["match_type"] = "exact"
    return extraction
This stage leverages Python's optimized string searching (likely Boyer-Moore or similar) to quickly determine if the extracted text appears verbatim in the source. This check costs O(n*m) in worst case but typically performs much better with early termination. Exact matches are immediately accepted and tagged with match_type "exact", as they represent the highest confidence extractions.
Stage 2: Configuration Gate
Before proceeding to fuzzy matching, the method checks the configuration:
pythonif matching_config.get("exact_only", False):
    return None
This gate allows users to enforce strict validation when precision is more important than recall, such as in regulatory compliance scenarios where only verbatim extractions are acceptable.
Stage 3: Tokenization Layer
If fuzzy matching is enabled, both the extraction and source chunk undergo tokenization through _get_tokens. This method first checks the TokenCache to avoid redundant tokenization of frequently seen text:
pythondef _get_tokens(self, text: str) -> List[str]:
    cached = self.token_cache.get(text)
    if cached is not None:
        return cached
    
    tokens = [self._normalize_token(t) for t in text.lower().split()]
    self.token_cache.put(text, tokens)
    return tokens
The tokenization process splits on whitespace and applies normalization through _normalize_token, which performs lowercasing and light stemming (removing plural 's' endings). This normalization is cached using Python's @lru_cache decorator with a 10,000 entry limit, preventing repeated processing of common words.
The TokenCache itself implements an LRU eviction policy using a deque to track access order, ensuring that frequently accessed chunks remain cached while rarely used ones are evicted when the cache reaches its 100-entry limit.
Stage 4: Progressive Threshold Fuzzy Matching
The fuzzy matching stage implements multiple passes with increasingly relaxed thresholds:
pythonfuzzy_passes = matching_config.get("fuzzy_passes", 1)
for pass_num in range(1, fuzzy_passes + 1):
    threshold_key = f"fuzzy_threshold_{pass_num}"
    threshold = matching_config.get(threshold_key, 0.75)
The default configuration specifies three passes with thresholds of 0.90, 0.70, and 0.50, allowing the system to first look for near-perfect matches before accepting increasingly approximate ones. This progressive relaxation prevents false positives while maximizing recall.
The Sliding Window Token Matcher
The _token_based_match method implements the core fuzzy matching algorithm. Rather than comparing the entire source text against the extraction, it uses a sliding window approach:
pythonfor window_size in range(len_e, min(len_e * 2, len_s) + 1):
    window_deque = deque(source_tokens[:window_size])
    window_counts = collections.Counter(window_deque)
The window starts at the size of the extraction and grows up to twice that size, acknowledging that LLM extractions might be condensed versions of longer source passages. For each window position, the method:

Pre-filters with token intersection: Computes the intersection of token counts between extraction and window using Counter arithmetic (extract_counts & window_counts). This O(n) operation quickly identifies windows with insufficient token overlap.
Applies SequenceMatcher only when promising: Only when token overlap exceeds the threshold does it compute the expensive SequenceMatcher ratio, which performs true sequence alignment considering token order.
Implements early termination: Returns immediately upon finding a match with ratio ≥ 0.95, as such high similarity indicates a definitive match.
Efficiently slides the window: Updates the window by removing the leftmost token and adding a new rightmost token, maintaining the Counter incrementally rather than rebuilding it.

Optimization Through Caching and Counting
The system employs several optimization strategies:

Token-level caching: The TokenCache prevents re-tokenization of chunks that appear multiple times across documents or entities
Normalization caching: The @lru_cache on _normalize_token prevents repeated processing of common words
Counter-based pre-filtering: Using Python's Counter for fast intersection computation eliminates most expensive sequence matching operations
Sliding window with incremental updates: The deque-based window with incremental Counter updates avoids O(n) rebuilds on each slide

Integration with Chunking Strategy
The matching system's effectiveness depends heavily on the chunking strategy. Page-based chunking with signature page combination ensures that related information (parties and their signatures) remains in the same chunk, increasing the likelihood of successful matching. Character-based chunking with overlap ensures that information spanning chunk boundaries can still be matched in at least one chunk.
The chunk size directly impacts matching performance: larger chunks increase the search space for the sliding window but provide more context for validation. The default 1000-character chunks with 100-character overlap represents a balance between context preservation and computational efficiency.
Handling Edge Cases
The system handles several edge cases:

Structured extractions without text field: For entities that return structured data (like signing_party with its parties array), the method bypasses matching entirely and accepts the extraction with match_type "extracted"
Empty extractions: Returns None immediately without attempting matching
Source text shorter than extraction: The quick check if len_s < len_e * threshold prevents futile matching attempts
Multiple valid windows: Continues searching even after finding a match to potentially find better matches

This progressive matching pipeline ensures that only validated extractions make it into the final results, maintaining data quality while accommodating the natural variations in LLM outputs. The multi-stage approach with increasing computational cost at each stage optimizes performance for the common case (exact matches) while still handling complex approximate matching when needed.


Overall Architecture and Design Philosophy
Xtract is designed as a configurable document processing pipeline that follows a clear separation of concerns. The framework uses YAML-based configuration to define extraction entities and evaluation metrics, allowing users to add new extraction capabilities without modifying code. The design emphasizes flexibility, performance optimization through caching and batching, and support for both local processing and distributed Spark environments.
Execution Flow and Component Analysis
1. Configuration Loading (config_loader.py)
The execution begins with the configuration system. The load_yaml function reads YAML configuration files and parses them into Python dictionaries. The load_config function serves as a convenience wrapper that loads the main settings file. The _resolve_env_vars function implements environment variable substitution, allowing users to reference environment variables in their configuration files using ${VARIABLE_NAME} syntax. This design choice enables secure configuration management by keeping sensitive information like API keys in environment variables rather than hardcoded in configuration files.
Input: YAML file path and optional environment variable references
Output: Resolved configuration dictionary with environment variables substituted
Purpose: Provides a flexible configuration system that supports both static and dynamic configuration values
2. Data Input and Parsing (io.py)
The data ingestion layer handles reading CSV files containing document data. The read_csv function serves as the main entry point, automatically detecting whether to use pandas or Spark based on the specified mode. The read_csv_pandas function reads CSV files using pandas, ensuring required columns exist and parsing JSON-formatted columns for paragraphs and tables. The read_csv_spark function provides Spark DataFrame support for large-scale processing, keeping JSON columns as strings for efficient distributed processing.
Input: CSV file path and processing mode (local/databricks)
Output: DataFrame (pandas or Spark) with parsed document structure
Purpose: Handles the initial data loading and parsing, supporting both local and distributed processing modes
3. Document Chunking (chunker.py)
The chunking system processes documents into manageable pieces for extraction. The Chunker class initializes with configuration parameters including strategy (page-based or character-based), maximum character limits, and overlap settings. The chunk method determines the chunking strategy, while _chunk_by_page implements intelligent page-based chunking with signature page detection. This method identifies signature pages using keyword matching and combines them with the first page for context, as signature pages often contain crucial party information that should be processed together with initial definitions.
Input: List of paragraph and table objects with page numbers
Output: List of text chunks optimized for extraction
Purpose: Breaks documents into logical units while preserving context and ensuring signature information is properly grouped
4. Entity Factory and Configuration (entity_factory.py)
The entity factory dynamically loads extraction entities from YAML configuration files. The EntityFactory class scans the extraction directory for entity definitions and loads evaluation configurations. Each Entity instance contains prompt templates, context variables, and evaluation settings. The format_prompt method substitutes context variables into prompt templates, allowing for dynamic prompt generation based on the specific entity and content being processed.
Input: Directory paths for extraction and evaluation configurations
Output: List of configured Entity objects with their prompts and evaluation settings
Purpose: Enables users to define new extraction entities through configuration files without code changes
5. Model Provider and Inference (providers.py)
The model provider system handles LLM interactions with automatic provider selection. The ModelProvider class determines whether to use the advanced router-based system or simple OpenAI access based on available configuration. The ModelRouter class implements intelligent routing based on model name patterns, automatically selecting appropriate providers (OpenAI, Azure OpenAI) for different model types. The OpenAIProvider and AzureOpenAIProvider classes handle the actual API calls with proper error handling and configuration management.
Input: Prompt text and optional model overrides
Output: LLM-generated text responses
Purpose: Provides a unified interface for different LLM providers while handling authentication, rate limiting, and response processing
6. Core Extraction Pipeline (extractor.py)
The main extraction engine orchestrates the entire process. The Extractor class initializes all components and manages the extraction workflow. The extract method determines the processing mode (pandas or Spark) and delegates to appropriate extraction methods. The _process_row method handles individual document processing, implementing intelligent batching when the number of chunks exceeds the configured threshold. The _progressive_match method implements the multi-pass matching strategy, starting with exact matches and progressively relaxing thresholds for fuzzy matching.
Input: DataFrame with document data and list of entity configurations
Output: Enhanced DataFrame with extraction results and evaluation scores
Purpose: Coordinates the entire extraction process, implementing performance optimizations and quality control measures
7. Progressive Matching and Token Caching
The extraction system implements sophisticated matching algorithms with performance optimizations. The TokenCache class provides LRU caching for frequently accessed tokenized text, reducing repeated processing overhead. The _progressive_match method implements a three-stage matching strategy: exact text matching, configurable exact-only mode, and progressive fuzzy matching with decreasing thresholds (90%, 70%, 50%). The _token_based_match method uses sliding window optimization and pre-computed token counts for efficient fuzzy matching, only computing expensive SequenceMatcher ratios when necessary.
Input: Extracted text and source content
Output: Matched extractions with confidence scores and match types
Purpose: Ensures high-quality extractions while maintaining performance through intelligent caching and optimization
8. Evaluation System (evaluator.py)
The evaluation pipeline assesses extraction quality using both rule-based and LLM-powered metrics. The Evaluator class loads evaluation configurations and determines which evaluations require LLM processing. The evaluate method applies all enabled evaluations to extraction results, creating new columns in the output DataFrame. Rule-based evaluations include exact matching and fuzzy similarity using SequenceMatcher, while LLM-based evaluations handle complex assessments like hallucination detection and completeness checking.
Input: DataFrame with extractions and evaluation configurations
Output: DataFrame with additional evaluation score columns
Purpose: Provides comprehensive quality assessment to validate extraction accuracy and identify potential issues
9. Output Generation and Persistence
The final stage handles result formatting and storage. The save_results function automatically detects the DataFrame type and uses appropriate storage methods. For pandas DataFrames, it converts complex objects to JSON strings for CSV compatibility. For Spark DataFrames, it provides multiple storage options including CSV, Parquet, and partitioned storage, with automatic partition management for large datasets.
Input: Enhanced DataFrame with extractions and evaluations
Output: Saved results in various formats (CSV, Parquet) with proper structure preservation
Purpose: Ensures results are stored in accessible formats while maintaining data integrity and supporting different deployment scenarios
Key Design Choices and Rationale
The framework's architecture reflects several key design decisions. The YAML-driven configuration system enables rapid prototyping and deployment without code changes, while the modular provider system allows easy integration of new LLM services. The progressive matching strategy balances accuracy with performance, using exact matches when possible and falling back to increasingly relaxed fuzzy matching. The token caching system optimizes repeated operations, while the batching capabilities enable efficient processing of large document collections.
The dual-mode operation (pandas/Spark) addresses different deployment scenarios, from local development to enterprise-scale processing. The evaluation system provides both immediate feedback through rule-based metrics and sophisticated analysis through LLM-powered assessments. The signature page detection in chunking demonstrates domain-specific optimization for legal documents, where signature pages often contain critical information that should be processed in context.
Overall, Xtract represents a well-architected framework that balances flexibility, performance, and maintainability while providing enterprise-grade document processing capabilities.