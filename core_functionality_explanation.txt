Deep Dive into Core Functions
_chunk_by_page() - Intelligent Document Segmentation
This function implements a sophisticated page-based chunking strategy specifically designed for legal documents. It takes lists of paragraph and table dictionaries as input, where each dictionary contains text content and metadata including page numbers.
The function begins by initializing empty dictionaries for page grouping and a set to track signature pages. It performs a first pass through all paragraphs searching for signature indicators using a predefined list of keywords including "IN WITNESS WHEREOF", "AGREED TO AND ACCEPTED", "/s/", and role identifiers like "Name:" and "Title:". Any page containing these keywords gets flagged as a signature page. This two-pass approach allows the function to identify all signature pages before constructing chunks, enabling intelligent recombination.
In the second pass, the function groups paragraphs by page number, concatenating all text from paragraphs and tables on the same page. The critical logic comes when handling signature pages: if signatures are found and include_signatures is enabled in configuration, the function combines the first page (containing party definitions and agreement context) with all signature pages into a single chunk. This design choice reflects the understanding that party identification often requires both the definition section and signature blocks to fully extract signing parties with their roles and representations.
The function outputs a list of text strings, where each string represents either a single page or the special combined first-page-plus-signatures chunk. This chunking strategy ensures that entity extraction models receive complete semantic units with necessary context, rather than fragmented information split across chunks.
_chunk_by_chars() - Sliding Window Text Segmentation
This alternative chunking method processes documents as continuous text streams when page boundaries aren't meaningful or available. It accepts the same paragraph list input but ignores page metadata.
The function concatenates all paragraph text into a single string, then implements a sliding window approach with configurable overlap. Starting at position 0, it extracts chunks of max_chars length (default 1000 characters). The overlap mechanism is crucial: instead of moving the window by the full chunk size, it moves by (chunk_size - overlap), ensuring that text near chunk boundaries appears in multiple chunks. This redundancy prevents the loss of entities that might span chunk boundaries.
The function continues sliding until it reaches the end of the text, with the final chunk potentially being shorter than max_chars. Empty or whitespace-only chunks are filtered out. The output is a list of overlapping text segments that cover the entire document. This approach trades increased processing (due to overlap) for improved recall, as entities won't be split mid-extraction.
_get_tokens() - Cached Text Tokenization
This function implements efficient tokenization with caching to avoid redundant processing of repeated text segments. It takes a text string as input and returns a list of normalized tokens.
The function first checks the TokenCache for previously tokenized versions of the input text. If found, it returns the cached result immediately, avoiding recomputation. For new text, it performs simple whitespace-based tokenization (splitting on spaces, tabs, newlines), then normalizes each token using the _normalize_token function. The normalized tokens are stored in the cache before returning.
The caching strategy is critical for performance because documents often contain repetitive sections (boilerplate language, standard clauses) that would otherwise be tokenized multiple times. The cache uses an LRU eviction policy to bound memory usage while keeping frequently accessed tokens available.
_normalize_token() - Token Standardization
This function, decorated with @lru_cache(maxsize=10000), normalizes individual tokens for consistent matching. It takes a single token string and returns its normalized form.
The normalization process involves converting to lowercase and stripping whitespace. Additionally, it implements light stemming by removing trailing 's' from words longer than 3 characters (unless they end in 'ss', avoiding false stemming of words like "class" or "across"). This stemming helps match singular/plural variations without the complexity of full morphological analysis.
The LRU cache decoration means the function maintains a dictionary of the 10,000 most recently processed tokens and their normalized forms. Given that documents typically have limited vocabulary, this cache size ensures most tokens are processed only once, providing O(1) lookup for repeated tokens.
TokenCache Class - LRU Token Cache Management
This class implements a least-recently-used cache specifically for tokenized text segments. It maintains both a dictionary for storage and a deque for tracking access order.
The get() method checks if requested text exists in the cache. If found, it removes the text from its current position in the access order deque and appends it to the end (marking it as most recently used), then returns the cached tokens. This O(n) deque operation is acceptable because the cache size is limited (default 100 entries).
The put() method adds new tokenized text to the cache. If the cache is at capacity, it removes the least recently used entry (leftmost in the deque) before adding the new entry. This ensures the cache maintains a bounded size while keeping the most useful entries.
_progressive_match() - Multi-Stage Extraction Validation
This is the core validation function that determines whether an LLM's extraction actually appears in the source text. It takes an extraction dictionary (containing the extracted text and metadata) and the source text chunk, returning either the validated extraction with match metadata or None if no match is found.
The function first handles special cases where extractions contain an "extractions" array (used for multi-value extractions like multiple signing parties). If this array exists and contains data, it's immediately accepted as valid without text matching, as these represent structured extractions that don't require validation against source text.
For text-based extractions, the function implements a three-stage matching pipeline:
Stage 1 performs exact substring matching using Python's in operator. This is the fastest check and catches cases where the LLM returned verbatim text. If successful, the extraction is tagged with match_type: "exact" and returned immediately.
Stage 2 checks if the configuration specifies exact_only: true. If so, the function returns None, enforcing strict extraction requirements. This is useful for high-precision scenarios where paraphrasing is unacceptable.
Stage 3 implements token-based fuzzy matching with multiple passes. The function tokenizes both the extraction and source text (using the cached tokenization functions), then attempts matching with progressively relaxed thresholds. Each pass uses a different threshold from configuration (e.g., fuzzy_threshold_1: 0.90, fuzzy_threshold_2: 0.70), allowing the system to prefer high-confidence matches while still catching lower-confidence valid extractions.
_token_based_match() - Optimized Sequence Matching
This function implements the core fuzzy matching algorithm, finding the best match between extracted tokens and source tokens. It takes two token lists and a threshold, returning a similarity ratio between 0 and 1.
The function begins with optimization checks: if either list is empty, it returns 0. If the source is shorter than the extraction multiplied by the threshold, it cannot possibly contain a good match and returns 0 immediately. These early exits prevent unnecessary computation.
The sliding window approach is key to efficiency. Instead of comparing the extraction against every possible subsequence of the source (O(n²)), it uses windows of varying sizes. For each window size (starting from the extraction length up to twice that length or the source length), it slides through the source text.
The critical optimization uses Counter objects for rapid intersection checking. Before computing expensive SequenceMatcher ratios, the function computes the token overlap between the extraction and current window using counter intersection (extract_counts & window_counts). Only if this overlap exceeds the minimum required tokens does it proceed to full sequence matching.
The window sliding is optimized using a deque and incremental counter updates. As the window slides, it removes the leftmost token from the counter and adds the new rightmost token, maintaining O(1) updates rather than reconstructing the counter each time.
The function includes early termination: if it finds a match with ratio ≥ 0.95, it returns immediately rather than searching for marginally better matches. This reflects the understanding that very high confidence matches don't benefit from further optimization.
_process_row() - Row-Level Extraction Pipeline
This function processes a single document (represented as a row) for a specific entity type. It takes a row dictionary containing paragraphs and tables, plus an entity configuration object.
The function first handles potential JSON string inputs, parsing them into Python objects if necessary. It then chunks the document using the configured chunker, which may use page-based or character-based strategies. If no chunks are produced (empty document), it returns an empty list.
The function implements intelligent batching logic: if the number of chunks exceeds a configured threshold (default 10) and batching is enabled, it formats all prompts at once and sends them to the model's batch inference endpoint. Otherwise, it processes chunks individually. This adaptive approach balances latency and throughput based on document size.
For each chunk, the function formats a prompt using the entity's template (which may include specific instructions for extracting that entity type), sends it to the model, parses the response, and validates it using progressive matching. Only validated extractions are included in the results list, providing quality control at the extraction level.
Design Interconnections
The tokenization, chunking, and fuzzy matching systems work together to create a robust extraction pipeline. Chunking ensures the LLM sees semantically complete units (like full pages or page combinations for contracts), maximizing extraction accuracy. The progressive matching system then validates these extractions, with tokenization and caching making fuzzy matching computationally feasible even for large documents with many chunks.
The multi-pass fuzzy matching with decreasing thresholds reflects a key insight: extraction quality varies, and a system that only accepts perfect matches (high threshold) will miss valid but reformatted extractions, while one that accepts everything (low threshold) will include false positives. The progressive approach finds a middle ground, preferring high-quality matches but accepting lower-quality ones when necessary.
The caching strategies (both token-level and text-level) recognize that document processing involves significant repetition - the same boilerplate appears across documents, similar chunks are processed for multiple entities, and common tokens appear thousands of times. By caching aggressively with bounded memory usage, the system achieves near-linear scaling with document size rather than quadratic scaling that naive implementations would exhibit.